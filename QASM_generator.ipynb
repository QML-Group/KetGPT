{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pre processing data"
      ],
      "metadata": {
        "id": "UbxCC2FmCnrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install prerequisites (for Colab use)\n",
        "\n",
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fkDal2nuCXuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torchtext.datasets as datasets\n",
        "from copy import deepcopy\n",
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "id": "XE_P-r0yEdU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset that is stored in the 'dataset' folder\n",
        "full_paths = []\n",
        "all_lines_list = []\n",
        "for dirpath, dirnames, filenames in os.walk('dataset'):\n",
        "    for f in filenames:\n",
        "        full_path = os.path.join(dirpath, f)\n",
        "        for line in open(full_path):\n",
        "            all_lines_list.append(line)\n",
        "\n",
        "# Trim dataset\n",
        "all_lines_unique_list = []\n",
        "all_lines_set = set(all_lines_list)\n",
        "for lines in all_lines_set:\n",
        "    if all_lines_list.count(lines) > 0: # How many times should the line be in the dataset to make it into vocabulary\n",
        "        all_lines_unique_list.append(lines)\n",
        "\n",
        "print('There are {} lines in our vocabulary'.format(len(all_lines_unique_list)))"
      ],
      "metadata": {
        "id": "Z1VLdtJTEtpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some helper functions\n",
        "\n",
        "\n",
        "def tokenizer_boran(vocab, qasm_file): # Simple tokenizer used in pre-processing the data\n",
        "  vector = []\n",
        "  for line in qasm_file:\n",
        "    try:\n",
        "      index = vocab.index(line)\n",
        "      vector.append(index)\n",
        "    except:\n",
        "      vector.append(-1)\n",
        "  return vector\n",
        "\n",
        "def detokenizer_boran(vocab, vector):\n",
        "  qasm_file = []\n",
        "  for element in vector:\n",
        "    qasm_file.append(vocab[element])\n",
        "\n",
        "  return qasm_file"
      ],
      "metadata": {
        "id": "ZVcCTbzZFW2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only count files that have no unknown lines\n",
        "\n",
        "all_lines_unique_list_unknown = deepcopy(all_lines_unique_list)\n",
        "all_lines_unique_list_unknown.append('unk')\n",
        "\n",
        "no_unk_list = [] # List of all lines without duplicates or unknowns\n",
        "counter = 0\n",
        "for dirpath, dirnames, filenames in os.walk('dataset'):\n",
        "  \n",
        "  for f in filenames:\n",
        "      full_path = os.path.join(dirpath, f)\n",
        "      qasm_file = []\n",
        "      for line in open(full_path):\n",
        "        qasm_file.append(line)\n",
        "      vector = tokenizer_boran(all_lines_unique_list_unknown,qasm_file)\n",
        "      detokenized_qasm_file = detokenizer_boran(all_lines_unique_list_unknown, vector)\n",
        "      if 'unk' not in detokenized_qasm_file:\n",
        "        no_unk_list.append(detokenized_qasm_file) #Currently not working\n",
        "      counter += 1\n",
        "      if not counter%100:\n",
        "       print('We are now at item {} of {}'.format(counter, len(filenames)))"
      ],
      "metadata": {
        "id": "d5MDaXzYFuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final definition of the dataset\n",
        "\n",
        "data_dictionary = {\"a\" : no_unk_list}\n",
        "\n",
        "dataset = Dataset.from_dict(data_dictionary)\n",
        "\n",
        "with open('data.txt', 'w') as f:\n",
        "  for line in all_lines_unique_list:\n",
        "    f.write(\"%s\" % line)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files = ['data.txt']) # Dataset can be loaded like this whenever needed"
      ],
      "metadata": {
        "id": "KFfVYyfSG637"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Tokenizer"
      ],
      "metadata": {
        "id": "YaD-WndoHa7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant imports\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "from typing import TYPE_CHECKING, List, Optional, Tuple\n",
        "import regex as re\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "from transformers.utils import logging\n",
        "if TYPE_CHECKING:\n",
        "    from transformers.pipelines.conversational import Conversation"
      ],
      "metadata": {
        "id": "TDDmXxgRHe8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining my own GPT2Tokenizer class to circumvent BPE tokenisation\n",
        "# Most code is from the Huggingface implementation\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\n",
        "    \"vocab_file\": \"vocab.json\",\n",
        "    \"merges_file\": \"merges.txt\",\n",
        "}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/vocab.json\",\n",
        "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/vocab.json\",\n",
        "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/vocab.json\",\n",
        "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/vocab.json\",\n",
        "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/vocab.json\",\n",
        "    },\n",
        "    \"merges_file\": {\n",
        "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/merges.txt\",\n",
        "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/merges.txt\",\n",
        "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/merges.txt\",\n",
        "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/merges.txt\",\n",
        "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/merges.txt\",\n",
        "    },\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"gpt2\": 1024,\n",
        "    \"gpt2-medium\": 1024,\n",
        "    \"gpt2-large\": 1024,\n",
        "    \"gpt2-xl\": 1024,\n",
        "    \"distilgpt2\": 1024,\n",
        "}\n",
        "\n",
        "class GPT2Tokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
        "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
        "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
        "    ```\n",
        "    >>> from transformers import GPT2Tokenizer\n",
        "    >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    >>> tokenizer(\"Hello world\")['input_ids']\n",
        "    [15496, 995]\n",
        "    >>> tokenizer(\" Hello world\")['input_ids']\n",
        "    [18435, 995]\n",
        "    ```\n",
        "    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
        "    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
        "    <Tip>\n",
        "    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
        "    </Tip>\n",
        "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
        "    this superclass for more information regarding those methods.\n",
        "    Args:\n",
        "        vocab_file (`str`):\n",
        "            Path to the vocabulary file.\n",
        "        merges_file (`str`):\n",
        "            Path to the merges file.\n",
        "        errors (`str`, *optional*, defaults to `\"replace\"`):\n",
        "            Paradigm to follow when decoding bytes to UTF-8. See\n",
        "            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
        "        unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
        "            token instead.\n",
        "        bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The beginning of sequence token.\n",
        "        eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The end of sequence token.\n",
        "        add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
        "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
        "            other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        merges_file,\n",
        "        errors=\"replace\",\n",
        "        unk_token=\"<|endoftext|>\",\n",
        "        bos_token=\"<|endoftext|>\",\n",
        "        eos_token=\"<|endoftext|>\",\n",
        "        pad_token=None,\n",
        "        add_prefix_space=False,\n",
        "        add_bos_token=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n",
        "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n",
        "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n",
        "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n",
        "        super().__init__(\n",
        "            errors=errors,\n",
        "            unk_token=unk_token,\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            pad_token=pad_token,\n",
        "            add_prefix_space=add_prefix_space,\n",
        "            add_bos_token=add_bos_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "        self.add_bos_token = add_bos_token\n",
        "\n",
        "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
        "            self.encoder = json.load(vocab_handle)\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = self.encoder\n",
        "        self.byte_decoder = self.decoder\n",
        "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
        "            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n",
        "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.add_prefix_space = add_prefix_space\n",
        "\n",
        "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.encoder)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.encoder, **self.added_tokens_encoder)\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        if self.add_bos_token:\n",
        "            bos_token_ids = [self.bos_token_id]\n",
        "        else:\n",
        "            bos_token_ids = []\n",
        "\n",
        "        output = bos_token_ids + token_ids_0\n",
        "\n",
        "        if token_ids_1 is None:\n",
        "            return output\n",
        "\n",
        "        return output + bos_token_ids + token_ids_1\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
        "        Args:\n",
        "            token_ids_0 (`List[int]`):\n",
        "                List of IDs.\n",
        "            token_ids_1 (`List[int]`, *optional*):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
        "                Whether or not the token list is already formatted with special tokens for the model.\n",
        "        Returns:\n",
        "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        "        if already_has_special_tokens:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
        "            )\n",
        "\n",
        "        if not self.add_bos_token:\n",
        "            return super().get_special_tokens_mask(\n",
        "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=False\n",
        "            )\n",
        "\n",
        "        if token_ids_1 is None:\n",
        "            return [1] + ([0] * len(token_ids_0))\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize a string.\"\"\"\n",
        "        bpe_tokens = []\n",
        "        #for token in re.findall(self.pat, text): #This was the huggingface implementation\n",
        "        #    token = \"\".join(\n",
        "        #        self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
        "        #    )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
        "        #    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
        "        bpe_token = self.encoder[text]\n",
        "        bpe_tokens.extend([bpe_token])\n",
        "        return bpe_tokens\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
        "        return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.decoder.get(index)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
        "        text = \"\".join(tokens)\n",
        "        text = self.byte_decoder[text]\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      return [self.encoder.get(tokens[0], self.encoder.get(self.unk_token))]\n",
        "\n",
        "\n",
        "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
        "            return\n",
        "        vocab_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
        "        )\n",
        "        merge_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n",
        "        )\n",
        "\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        index = 0\n",
        "        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(\"#version: 0.2\\n\")\n",
        "            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n",
        "                        \" Please check that the tokenizer is not corrupted!\"\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return vocab_file, merge_file\n",
        "\n",
        "    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n",
        "        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n",
        "        if is_split_into_words or add_prefix_space:\n",
        "            text = \" \" + text\n",
        "        return (text, kwargs)\n",
        "\n",
        "    def _build_conversation_input_ids(self, conversation: \"Conversation\") -> List[int]:\n",
        "        input_ids = []\n",
        "        for is_user, text in conversation.iter_texts():\n",
        "            input_ids.extend(self.encode(text, add_special_tokens=False) + [self.eos_token_id])\n",
        "        if len(input_ids) > self.model_max_length:\n",
        "            input_ids = input_ids[-self.model_max_length :]\n",
        "        return input_ids"
      ],
      "metadata": {
        "id": "ZxzPDwV5ISL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"training\" the tokenizer by creating a vocab.json file\n",
        "\n",
        "!mkdir gpt_tokenizer # Ask Colab to create gpt_tokenizer folder\n",
        "\n",
        "with open('data.txt') as f: # lines object is all lines of interest\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "with open('gpt_tokenizer/vocab.json', 'w') as f: # Create vocab.json\n",
        "  f.write(\"{\")\n",
        "  f.write('\"<s>\":0,\"<pad>\":1,\"</s>\":2,\"<unk>\":3,\"<mask>\":4')\n",
        "  for i, line in enumerate(lines):\n",
        "    f.write(\",\\\"{}\\\":{}\".format(line,i+5)) # Contains a bug: include \"qelib\" not dealt with correctly due to \"\", fix later\n",
        "\n",
        "  f.write(\"}\")\n",
        "\n",
        "open('gpt_tokenizer/merges.txt', 'w') # Create empty merges.txt"
      ],
      "metadata": {
        "id": "64bnTPo-IiZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt_tokenizer')\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "4fgyrYGbH6CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and training the model"
      ],
      "metadata": {
        "id": "UlhtBRr5I2Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# relevant imports\n",
        "\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, DataCollatorForLanguageModeling, \\\n",
        "                          Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "twTViqTZI6tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size = tokenizer.vocab_size,\n",
        "    bos_token = tokenizer.bos_token_id,\n",
        "    eos_token = tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "btFK5nLQMAEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training\n",
        "\n",
        "def encode(lines):\n",
        "  return tokenizer(lines['text'], add_special_tokens=True, max_length = 1024)\n",
        "\n",
        "\n",
        "dataset.set_transform(encode)\n",
        "dataset = dataset['train']\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "f2Y9s81UMShK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments and train the model\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"model\",\n",
        "    overwrite_output_dir = True,\n",
        "    num_train_epochs = 1000,\n",
        "    per_device_train_batch_size = 4,\n",
        "    save_steps = 1000,\n",
        "    save_total_limit = 2,\n",
        "    prediction_loss_only = True,\n",
        "    remove_unused_columns = False\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = dataset\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"trained_model\")"
      ],
      "metadata": {
        "id": "7kr5iDHCMrR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate qasm files"
      ],
      "metadata": {
        "id": "pp-hrfaqNIVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new data with model\n",
        "\n",
        "def detokenizer_boran_hack(tokens): # Hacky solution, use a nicer solution in the future\n",
        "  decoded_output = []\n",
        "  for token in tokens:\n",
        "    if token > 5:\n",
        "     decoded_output.append(lines[token - 5])\n",
        "  return decoded_output\n",
        "\n",
        "# Generate using beam search\n",
        "beam_output = model.generate(input_ids = tokenizer.encode('OPENQASM 2.0 \\n', return_tensors = \"pt\").to('cuda'),\n",
        "                       max_length = 512,\n",
        "                       num_beams = 10,\n",
        "                       no_repeat_ngram_size = 5,\n",
        "                       num_return_sequences = 1)\n",
        "\n",
        "decoded_output = detokenizer_boran_hack(beam_output[0])\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "zoNGkdRVNaRd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}