{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pre processing data"
      ],
      "metadata": {
        "id": "UbxCC2FmCnrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install prerequisites (for Colab use)\n",
        "\n",
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fkDal2nuCXuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General imports\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import log_softmax, pad\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import torchtext.datasets as datasets\n",
        "from copy import deepcopy\n",
        "from datasets import Dataset, load_dataset"
      ],
      "metadata": {
        "id": "XE_P-r0yEdU3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset that is stored in the 'dataset' folder\n",
        "\n",
        "full_paths = []\n",
        "all_lines_list = []\n",
        "for dirpath, dirnames, filenames in os.walk('dataset'):\n",
        "    for f in filenames:\n",
        "        full_path = os.path.join(dirpath, f)\n",
        "        for line in open(full_path):\n",
        "            all_lines_list.append(line)\n",
        "\n",
        "# Trim dataset\n",
        "all_lines_unique_list = []\n",
        "all_lines_set = set(all_lines_list)\n",
        "for lines in all_lines_set:\n",
        "    if all_lines_list.count(lines) > 0: # How many times should the line be in the dataset to make it into vocabulary\n",
        "        all_lines_unique_list.append(lines)\n",
        "\n",
        "print('There are {} lines in our vocabulary'.format(len(all_lines_unique_list)))"
      ],
      "metadata": {
        "id": "Z1VLdtJTEtpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some helper functions\n",
        "\n",
        "def tokenizer_boran(vocab, qasm_file): # Simple tokenizer used in pre-processing the data\n",
        "  vector = []\n",
        "  for line in qasm_file:\n",
        "    try:\n",
        "      index = vocab.index(line)\n",
        "      vector.append(index)\n",
        "    except:\n",
        "      vector.append(-1)\n",
        "  return vector\n",
        "\n",
        "def detokenizer_boran(vocab, vector):\n",
        "  qasm_file = []\n",
        "  for element in vector:\n",
        "    qasm_file.append(vocab[element])\n",
        "\n",
        "  return qasm_file"
      ],
      "metadata": {
        "id": "ZVcCTbzZFW2T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only count files that have no unknown lines\n",
        "\n",
        "all_lines_unique_list_unknown = deepcopy(all_lines_unique_list)\n",
        "all_lines_unique_list_unknown.append('unk')\n",
        "\n",
        "no_unk_list = [] # List of all lines without duplicates or unknowns\n",
        "counter = 0\n",
        "for dirpath, dirnames, filenames in os.walk('dataset'):\n",
        "  \n",
        "  for f in filenames:\n",
        "      full_path = os.path.join(dirpath, f)\n",
        "      qasm_file = []\n",
        "      for line in open(full_path):\n",
        "        qasm_file.append(line)\n",
        "      vector = tokenizer_boran(all_lines_unique_list_unknown,qasm_file)\n",
        "      detokenized_qasm_file = detokenizer_boran(all_lines_unique_list_unknown, vector)\n",
        "      if 'unk' not in detokenized_qasm_file:\n",
        "        no_unk_list.append(detokenized_qasm_file)\n",
        "      counter += 1\n",
        "      if not counter%100:\n",
        "       print('We are now at item {} of {}'.format(counter, len(filenames)))"
      ],
      "metadata": {
        "id": "d5MDaXzYFuY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final definition of the dataset\n",
        "\n",
        "with open('data.txt', 'w') as f:\n",
        "  for counter, line in enumerate(no_unk_list):\n",
        "    if counter != 0:\n",
        "      f.write(\"\\n\")\n",
        "    f.write(\"%s\" % line)\n",
        "\n",
        "dataset = load_dataset(\"text\", data_files = 'data.txt') # Dataset can be loaded like this whenever needed\n",
        "\n",
        "\n",
        "with open('data.txt', 'w') as f: # Rewrite data.txt file for other purpose. Change this later!\n",
        "  for line in all_lines_unique_list:\n",
        "    f.write(\"%s\" % line)"
      ],
      "metadata": {
        "id": "6MzT1YOxXX1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Tokenizer"
      ],
      "metadata": {
        "id": "YaD-WndoHa7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant imports\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
        "import json\n",
        "import os\n",
        "from functools import lru_cache\n",
        "from typing import TYPE_CHECKING, List, Optional, Tuple\n",
        "import regex as re\n",
        "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
        "from transformers.utils import logging\n",
        "if TYPE_CHECKING:\n",
        "    from transformers.pipelines.conversational import Conversation"
      ],
      "metadata": {
        "id": "TDDmXxgRHe8I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining my own GPT2Tokenizer class to circumvent BPE tokenisation\n",
        "# Most code is from the Huggingface implementation\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\n",
        "    \"vocab_file\": \"vocab.json\",\n",
        "    \"merges_file\": \"merges.txt\",\n",
        "}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/vocab.json\",\n",
        "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/vocab.json\",\n",
        "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/vocab.json\",\n",
        "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/vocab.json\",\n",
        "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/vocab.json\",\n",
        "    },\n",
        "    \"merges_file\": {\n",
        "        \"gpt2\": \"https://huggingface.co/gpt2/resolve/main/merges.txt\",\n",
        "        \"gpt2-medium\": \"https://huggingface.co/gpt2-medium/resolve/main/merges.txt\",\n",
        "        \"gpt2-large\": \"https://huggingface.co/gpt2-large/resolve/main/merges.txt\",\n",
        "        \"gpt2-xl\": \"https://huggingface.co/gpt2-xl/resolve/main/merges.txt\",\n",
        "        \"distilgpt2\": \"https://huggingface.co/distilgpt2/resolve/main/merges.txt\",\n",
        "    },\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"gpt2\": 1024,\n",
        "    \"gpt2-medium\": 1024,\n",
        "    \"gpt2-large\": 1024,\n",
        "    \"gpt2-xl\": 1024,\n",
        "    \"distilgpt2\": 1024,\n",
        "}\n",
        "\n",
        "class GPT2Tokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "    Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
        "    This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
        "    be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
        "    ```\n",
        "    >>> from transformers import GPT2Tokenizer\n",
        "    >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    >>> tokenizer(\"Hello world\")['input_ids']\n",
        "    [15496, 995]\n",
        "    >>> tokenizer(\" Hello world\")['input_ids']\n",
        "    [18435, 995]\n",
        "    ```\n",
        "    You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
        "    call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
        "    <Tip>\n",
        "    When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
        "    </Tip>\n",
        "    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
        "    this superclass for more information regarding those methods.\n",
        "    Args:\n",
        "        vocab_file (`str`):\n",
        "            Path to the vocabulary file.\n",
        "        merges_file (`str`):\n",
        "            Path to the merges file.\n",
        "        errors (`str`, *optional*, defaults to `\"replace\"`):\n",
        "            Paradigm to follow when decoding bytes to UTF-8. See\n",
        "            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
        "        unk_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
        "            token instead.\n",
        "        bos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The beginning of sequence token.\n",
        "        eos_token (`str`, *optional*, defaults to `<|endoftext|>`):\n",
        "            The end of sequence token.\n",
        "        add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
        "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
        "            other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    #pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        merges_file,\n",
        "        errors=\"replace\",\n",
        "        unk_token=\"<|endoftext|>\",\n",
        "        bos_token=\"<|endoftext|>\",\n",
        "        eos_token=\"<|endoftext|>\",\n",
        "        pad_token=None,\n",
        "        add_prefix_space=False,\n",
        "        add_bos_token=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n",
        "        eos_token = AddedToken(eos_token, lstrip=False, rstrip=False) if isinstance(eos_token, str) else eos_token\n",
        "        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token\n",
        "        pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_token, str) else pad_token\n",
        "        super().__init__(\n",
        "            errors=errors,\n",
        "            unk_token=unk_token,\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            pad_token=pad_token,\n",
        "            add_prefix_space=add_prefix_space,\n",
        "            add_bos_token=add_bos_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "        self.add_bos_token = add_bos_token\n",
        "\n",
        "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
        "            self.encoder = json.load(vocab_handle)\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.errors = errors  # how to handle errors in decoding\n",
        "        self.byte_encoder = self.encoder\n",
        "        self.byte_decoder = self.decoder\n",
        "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
        "            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n",
        "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
        "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
        "        self.cache = {}\n",
        "        self.add_prefix_space = add_prefix_space\n",
        "\n",
        "        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
        "        #self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.encoder)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.encoder, **self.added_tokens_encoder)\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        if self.add_bos_token:\n",
        "            bos_token_ids = [self.bos_token_id]\n",
        "        else:\n",
        "            bos_token_ids = []\n",
        "\n",
        "        output = bos_token_ids + token_ids_0\n",
        "\n",
        "        if token_ids_1 is None:\n",
        "            return output\n",
        "\n",
        "        return output + bos_token_ids + token_ids_1\n",
        "\n",
        " #   def get_special_tokens_mask(\n",
        " #       self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        " #   ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
        "        Args:\n",
        "            token_ids_0 (`List[int]`):\n",
        "                List of IDs.\n",
        "            token_ids_1 (`List[int]`, *optional*):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
        "                Whether or not the token list is already formatted with special tokens for the model.\n",
        "        Returns:\n",
        "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        " #       if already_has_special_tokens:\n",
        " #           return super().get_special_tokens_mask(\n",
        " #               token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
        " #           )\n",
        "\n",
        "  #      if not self.add_bos_token:\n",
        "   #         return super().get_special_tokens_mask(\n",
        "    #            token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=False\n",
        "     #       )\n",
        "\n",
        "      #  if token_ids_1 is None:\n",
        "       #     return [1] + ([0] * len(token_ids_0))\n",
        "        #return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n",
        "\n",
        "    def tokenize_boran(self, text):\n",
        "        \"\"\"Tokenize a string.\"\"\"\n",
        "        #bpe_tokens = []\n",
        "        #for token in re.findall(self.pat, text): #This was the huggingface implementation\n",
        "        #    token = \"\".join(\n",
        "        #        self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
        "        #    )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n",
        "        #    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
        "        #bpe_token = self.encoder.get(text)\n",
        "        #bpe_tokens.extend([bpe_token])\n",
        "        bpe_tokens = text\n",
        "        try:\n",
        "          if text[0][0] == '[':\n",
        "            bpe_tokens = []\n",
        "            for subtext_index, subtext in enumerate(text):\n",
        "              text_replaced = text[subtext_index].replace(\"'\", \"\")\n",
        "              text_replaced = text_replaced.replace(\"\\\\n\", \"\\n\")\n",
        "              text_replaced = text_replaced[1:]\n",
        "              #bpe_tokens = text_replaced[:-1].split(', ')\n",
        "              bpe_tokens.append(text_replaced[:-1].split(', '))\n",
        "        except:\n",
        "          bpe_tokens = text\n",
        "\n",
        "        return bpe_tokens\n",
        "    def tokenize(self, text, is_split_into_words = True):\n",
        "      bpe_tokens = text\n",
        "      return text\n",
        "\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
        "        return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.decoder.get(index)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
        "        text = \"\".join(tokens)\n",
        "        text = self.byte_decoder[text]\n",
        "        return text\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        return tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      return self.encoder.get(tokens, self.encoder.get(self.unk_token))\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      try:\n",
        "        ids = [self.encoder.get(tokens)]\n",
        "        if tokens == '<pad>' or tokens == '<s>' or tokens == '</s>' or tokens == '<mask>':\n",
        "          ids = self.encoder.get(tokens)\n",
        "      except:\n",
        "        try:\n",
        "          ids = []\n",
        "          for token in tokens:\n",
        "            id = self.encoder.get(token)\n",
        "            ids.append(id)\n",
        "        except:\n",
        "          ids = []\n",
        "          ids_list = []\n",
        "          for lists in tokens:\n",
        "            for token in lists:\n",
        "              id = self.encoder.get(token)\n",
        "              ids.append(id)\n",
        "            ids_list.append(ids)\n",
        "          ids = ids_list\n",
        "      return ids\n",
        "\n",
        "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
        "            return\n",
        "        vocab_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
        "        )\n",
        "        merge_file = os.path.join(\n",
        "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"merges_file\"]\n",
        "        )\n",
        "\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        index = 0\n",
        "        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(\"#version: 0.2\\n\")\n",
        "            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        f\"Saving vocabulary to {merge_file}: BPE merge indices are not consecutive.\"\n",
        "                        \" Please check that the tokenizer is not corrupted!\"\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return vocab_file, merge_file\n",
        "\n",
        "    def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n",
        "        add_prefix_space = kwargs.pop(\"add_prefix_space\", self.add_prefix_space)\n",
        "        if is_split_into_words or add_prefix_space:\n",
        "            #text = \" \" + text\n",
        "            text = text\n",
        "        return (text, kwargs)\n",
        "\n",
        "    def _build_conversation_input_ids(self, conversation: \"Conversation\") -> List[int]:\n",
        "        input_ids = []\n",
        "        for is_user, text in conversation.iter_texts():\n",
        "            input_ids.extend(self.encode(text, add_special_tokens=False) + [self.eos_token_id])\n",
        "        if len(input_ids) > self.model_max_length:\n",
        "            input_ids = input_ids[-self.model_max_length :]\n",
        "        return input_ids\n"
      ],
      "metadata": {
        "id": "ZxzPDwV5ISL0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"training\" the tokenizer by creating a vocab.json file\n",
        "\n",
        "!mkdir gpt_tokenizer # Ask Colab to create gpt_tokenizer folder\n",
        "\n",
        "with open('data.txt') as f: # lines object is all lines of interest\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "with open('gpt_tokenizer/vocab.json', 'w') as f: # Create vocab.json\n",
        "  f.write(\"{\")\n",
        "  f.write('\"<s>\":0,\"<pad>\":1,\"</s>\":2,\"<unk>\":3,\"<mask>\":4')\n",
        "  for i, line in enumerate(lines):\n",
        "    f.write(\",\\\"{}\\\\n\\\":{}\".format(line.replace('\"','\\\\\"'),i+5))\n",
        "  f.write(\"}\")\n",
        "\n",
        "open('gpt_tokenizer/merges.txt', 'w') # Create empty merges.txt"
      ],
      "metadata": {
        "id": "64bnTPo-IiZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7fe27f-691c-4406-bb44-2430c285d3ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='gpt_tokenizer/merges.txt' mode='w' encoding='UTF-8'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the tokenizer\n",
        "# Minor bug: Prints a number for some reason\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt_tokenizer')\n",
        "tokenizer.add_special_tokens({\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"mask_token\": \"<mask>\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "4fgyrYGbH6CY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f99533-808e-46b2-acb0-b32fd76e7755"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test to see if our tokenizer works\n",
        "\n",
        "tokenizer('h q[0];\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty8-cIEm6ONH",
        "outputId": "d030446a-22d7-4b7b-edab-1925fd9a1cd2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [64], 'attention_mask': [1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and training the model"
      ],
      "metadata": {
        "id": "UlhtBRr5I2Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# relevant imports\n",
        "\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, DataCollatorForLanguageModeling, \\\n",
        "                          Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "twTViqTZI6tq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size = tokenizer.vocab_size,\n",
        "    bos_token = tokenizer.bos_token_id,\n",
        "    eos_token = tokenizer.eos_token_id,\n",
        "    n_embd = 128,\n",
        "    n_layer = 3,\n",
        "    n_head = 4\n",
        "\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)#.to('cuda')"
      ],
      "metadata": {
        "id": "btFK5nLQMAEO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training\n",
        "\n",
        "#def encode(lines): # Old method... I don't specify max length for the newer method, maybe this gives issues later?\n",
        "#  return tokenizer(lines['text'], add_special_tokens=True, max_length = 1024)\n",
        "\n",
        "def encode(lines):\n",
        "  return tokenizer._batch_encode_plus(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tokenizer.tokenize_boran(lines[\"text\"]))), is_split_into_words = True)\n",
        "\n",
        "dataset.set_transform(encode)\n",
        "dataset = dataset[\"train\"]\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "f2Y9s81UMShK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Tensorboard callback\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers.integrations import TensorBoardCallback\n",
        "import tensorflow as tf\n",
        "writer = SummaryWriter()\n",
        "callback = TensorBoardCallback(writer)\n",
        "logdir = \"logs\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "z_yWfbeh9NTY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments and train the model\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"model\",\n",
        "    overwrite_output_dir = True,\n",
        "    num_train_epochs = 50000,\n",
        "    per_device_train_batch_size = 4,\n",
        "    save_steps = 1000,\n",
        "    save_total_limit = 2,\n",
        "    prediction_loss_only = False,\n",
        "    remove_unused_columns = False,\n",
        "#    learning_rate = 1e-5,\n",
        "    logging_first_step = True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = dataset)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"trained_model\")"
      ],
      "metadata": {
        "id": "7kr5iDHCMrR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not working at the moment, haven't looked much into how to get this working yet\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"logs\""
      ],
      "metadata": {
        "id": "dmA7uleSMVkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate qasm files"
      ],
      "metadata": {
        "id": "pp-hrfaqNIVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new data with model\n",
        "\n",
        "def detokenizer_boran_hack(tokens): # Hacky solution, use a nicer solution in the future\n",
        "  decoded_output = []\n",
        "  for token in tokens:\n",
        "    if token > 5:\n",
        "     decoded_output.append(lines[token - 5])\n",
        "  return decoded_output\n",
        "\n",
        "# Generate using beam search\n",
        "beam_output = model.generate(input_ids = tokenizer(['OPENQASM 2.0;\\n', 'include \"qelib1.inc\";\\n', 'qreg q[19];\\n'], return_tensors = \"pt\").to('cuda')[\"input_ids\"],\n",
        "                       max_length = 89,\n",
        "                       num_beams = 20,\n",
        "                       no_repeat_ngram_size = 5,\n",
        "                       num_return_sequences = 3)\n",
        "\n",
        "decoded_output = detokenizer_boran_hack(beam_output[0])\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "id": "zoNGkdRVNaRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b62dfe3-0d55-477b-a773-ca394a9e7b12"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[19];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[7];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[7];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'h q[0];', 'h q[1];', 'h q[2];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[8];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the most likely output into a .qasm file\n",
        "\n",
        "with open('generated_qasm_file.qasm', 'w') as f:\n",
        "  for line in decoded_output:\n",
        "    f.write(\"%s\\n\" % line)"
      ],
      "metadata": {
        "id": "0QooTNMx3s4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miscellaneous"
      ],
      "metadata": {
        "id": "33DXDCyw33uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interesting output comparison using beam search (Only prompt was OPENQASM 2.0;)\n",
        "\n",
        "print(detokenizer_boran_hack(beam_output[0]))\n",
        "print(detokenizer_boran_hack(beam_output[1]))\n",
        "print(detokenizer_boran_hack(beam_output[2]))\n",
        "print(detokenizer_boran_hack(beam_output[3]))\n",
        "print(detokenizer_boran_hack(beam_output[4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MII_inoSOddS",
        "outputId": "9b3b9db7-d2ab-44f4-c069-04e34f2a73e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[17];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[16];', 'ccx q[0],q[1],q[9];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[0],q[1],q[6];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'x q[8];', 'x q[9];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[7];', 'x q[0];', 'x q[1];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[17];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[16];', 'ccx q[0],q[1],q[9];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[0],q[1],q[6];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'x q[8];', 'x q[9];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[7];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'h q[2];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[17];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[16];', 'ccx q[0],q[1],q[9];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[0],q[1],q[6];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'x q[8];', 'x q[9];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[5],q[9],q[10];', 'ccx q[4],q[8],q[9];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[9];', 'ccx q[5],q[9],q[10];', 'ccx q[4],q[8],q[9];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[0];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[17];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[16];', 'ccx q[0],q[1],q[9];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[0],q[1],q[6];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'x q[8];', 'x q[9];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'h q[2];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[17];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'h q[16];', 'ccx q[0],q[1],q[9];', 'ccx q[2],q[9],q[10];', 'ccx q[3],q[10],q[11];', 'ccx q[4],q[11],q[12];', 'ccx q[5],q[12],q[13];', 'ccx q[6],q[13],q[14];', 'ccx q[7],q[14],q[15];', 'ccx q[8],q[15],q[16];', 'ccx q[7],q[14],q[15];', 'ccx q[6],q[13],q[14];', 'ccx q[5],q[12],q[13];', 'ccx q[4],q[11],q[12];', 'ccx q[3],q[10],q[11];', 'ccx q[2],q[9],q[10];', 'ccx q[0],q[1],q[9];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'ccx q[0],q[1],q[6];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[8];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'x q[7];', 'x q[8];', 'x q[9];', 'h q[9];', 'h q[18];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[5],q[13],q[14];', 'ccx q[6],q[14],q[15];', 'ccx q[7],q[15],q[16];', 'ccx q[8],q[16],q[17];', 'ccx q[8],q[16],q[17];', 'ccx q[7],q[15],q[16];', 'ccx q[6],q[14],q[15];', 'ccx q[5],q[13],q[14];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'ccx q[2],q[10],q[11];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[4],q[12],q[13];', 'ccx q[3],q[11],q[12];', 'ccx q[2],q[10],q[11];', 'ccx q[0],q[1],q[10];', 'h q[0];', 'h q[1];', 'h q[2];', 'ccx q[6],q[12],q[7];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[7];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'h q[2];']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Interesting output comparison using beam search (Only prompt was OPENQASM 2.0;)\n",
        "\n",
        "print(detokenizer_boran_hack(beam_output[0]))\n",
        "print(detokenizer_boran_hack(beam_output[1]))\n",
        "print(detokenizer_boran_hack(beam_output[2]))\n",
        "print(detokenizer_boran_hack(beam_output[3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEFZ9w0jUQMN",
        "outputId": "b832a49c-ffff-47bf-df56-568b9e801d57"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[15];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[14];', 'ccx q[0],q[1],q[8];', 'ccx q[2],q[8],q[9];', 'ccx q[3],q[9],q[10];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'x q[6];', 'h q[6];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'x q[6];', 'h q[6];', 'ccx q[0],q[1],q[7];', 'ccx q[2],q[7],q[8];', 'ccx q[3],q[8],q[9];', 'ccx q[4],q[9],q[10];', 'ccx q[5],q[10],q[6];', 'ccx q[4],q[9],q[10];', 'ccx q[3],q[8],q[9];', 'ccx q[2],q[7],q[8];', 'ccx q[0],q[1],q[7];', 'h q[6];', 'x q[0];', 'x q[1];', 'x q[2];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[15];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[14];', 'ccx q[0],q[1],q[8];', 'ccx q[2],q[8],q[9];', 'ccx q[3],q[9],q[10];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[10];', 'OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[13];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[15];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[14];', 'ccx q[0],q[1],q[8];', 'ccx q[2],q[8],q[9];', 'ccx q[3],q[9],q[10];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[10];', 'OPENQASM 2.0;', 'include \"qelib1.inc\";', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];']\n",
            "['OPENQASM 2.0;', 'include \"qelib1.inc\";', 'qreg q[15];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];', 'h q[4];', 'h q[5];', 'h q[6];', 'h q[7];', 'h q[14];', 'ccx q[0],q[1],q[8];', 'ccx q[2],q[8],q[9];', 'ccx q[3],q[9],q[10];', 'ccx q[4],q[10],q[11];', 'ccx q[5],q[11],q[12];', 'ccx q[6],q[12],q[13];', 'ccx q[7],q[13],q[14];', 'ccx q[6],q[12],q[13];', 'ccx q[5],q[11],q[12];', 'ccx q[4],q[10],q[11];', 'ccx q[3],q[9],q[10];', 'ccx q[2],q[8],q[9];', 'ccx q[0],q[1],q[8];', 'h q[0];', 'h q[1];', 'ccx q[5],q[10],q[11];', 'ccx q[6],q[11],q[12];', 'ccx q[5],q[10],q[11];', 'ccx q[6],q[11],q[12];', 'ccx q[5],q[10],q[11];', 'ccx q[4],q[9],q[10];', 'ccx q[2],q[6],q[7];', 'ccx q[3],q[7],q[8];', 'ccx q[4],q[8],q[5];', 'ccx q[3],q[7],q[8];', 'ccx q[2],q[6],q[7];', 'ccx q[0],q[1],q[6];', 'h q[5];', 'x q[0];', 'x q[1];', 'x q[2];', 'x q[3];', 'x q[4];', 'x q[5];', 'h q[0];', 'h q[1];', 'h q[2];', 'h q[3];']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qiskit"
      ],
      "metadata": {
        "id": "LLRjUQzZUvV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if generated circuit can be loaded into qiskit\n",
        "\n",
        "import qiskit\n",
        "\n",
        "circ = qiskit.circuit.QuantumCircuit.from_qasm_file(r'generated_qasm_file.qasm')"
      ],
      "metadata": {
        "id": "-bRbH9ZpU9Qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}